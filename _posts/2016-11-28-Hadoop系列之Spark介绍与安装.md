---
layout: post
title: Hadoop系列之（五）Spark介绍与安装配置
categories:
- 大数据
---

<div class="message">
	已经约两周没有更新了，接下来会更新两三篇左右关于整个hadoop系统的安装配置，安装配置系列结束后开始进入实战原理篇。
</div>

## 一、简介

### 1.Spark介绍

Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类Hadoop MapReduce的通用并行框架，Spark，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。
Spark,是一种"One Stack to rule them all"的大数据计算框架，期望使用一个技术完美的解决大数据领域的各项计算任务。Apache官方对Spark的定义：通用的大数据快速处理引擎。Spark除了一站式的特点之外，另外一个最重要的特点就是基于内存进行计算，从而让它的速度可以达到MapReduce、Hive的数倍甚至数十倍。
Spark使用Scala语言进行实现，它是一种面向对象、函数式编程语言，能够像操作本地集合对象一样轻松地操作分布式数据集（Scala 提供一个称为 Actor 的并行模型，其中Actor通过它的收件箱来发送和接收非同步信息而不是共享数据，该方式被称为：Shared Nothing 模型）。在Spark官网上介绍，它具有运行速度快、易用性好、通用性强和随处运行等特点。
现在已经有很多大公司正在生产环境下深度地使用Spark作为大数据的计算框架，包括eBay、Yahoo!、BAT、网易、京东、华为、大众点评、优酷土豆、搜狗等。Spark也获得了多个世界顶级的IT厂商的支持，包括IBM、Intel等。

### 2.Spark特点

* `速度快`：Spark基于内存进行计算（随着数据量的增大，也有部分计算基于磁盘，如shuffle）。
* `易用性好`：Spark是基于RDD的计算模型，比Hadoop的基于MapReduce的计算模型更加易于理解和上手开发，实现各种复杂功能，比如二次排序、topn等复杂操作时，更加便捷。Spark不仅支持Scala编写应用程序，而且支持Java和Python等语言进行编写，特别是Scala是一种高效、可拓展的语言，能够用简洁的代码处理较为复杂的处理工作。
* `通用性强`：Spark提供了Spark RDD、Spark SQL、Spark Streaming、MLlib、GraphX等技术组件，可以一站式的完成数据领域的离线批处理、交互式查询、实时流计算、机器学习与图计算等重要的任务和问题。
* `集成Hadoop`：Spark并不是要成为一个大数据领域的"独裁者"，一个人霸占大数据领域的所有"地盘",而是与Hadoop进行了高度的集成，两者可以完美的配合使用。Hadoop的HDFS、Hive、HBase负责存储，YARN负责资源调度；Spark负责大数据计算。实际上Hadoop+Spark的组合，是一种"double win"的组合。

### 3.Spark适用场景

目前大数据处理场景有以下几个类型：

 * 复杂的批量处理（Batch Data Processing），偏重点在于处理海量数据的能力，至于处理速度可忍受，通常的时间可能是在数十分钟到数小时；

 * 基于历史数据的交互式查询（Interactive Query），通常的时间在数十秒到数十分钟之间

 * 基于实时数据流的数据处理（Streaming Data Processing），通常在数百毫秒到数秒之间

目前对以上三种场景需求都有比较成熟的处理框架：

 * 第一种情况可以用Hadoop的MapReduce来进行批量海量数据处理。
 * 第二种情况可以Impala进行交互式查询。
 * 第三种情况可以用Storm分布式处理框架处理实时流式数据。

以上三者都是比较独立，各自一套维护成本比较高，而Spark的出现能够一站式平台满意以上需求。 
通过以上分析，总结Spark场景有以下几个：

 * Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小
 * 由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如web服务的存储或者是增量的web爬虫和索引。就是对于那种增量修改的应用模型不适合数据量不是特别大，但是要求实时统计分析需求


## 二、Spark安装部署

### 1.Scala安装

#### (1).Scala解压安装

#### (2).环境变量配置

#### (3).将Scala复制到其他节点

#### (4).验证Scala

### 2.Spark安装

#### ().Spark解压安装

#### ().Spark配置

#### ().添加到环境变量

#### ().复制到其他节点

#### ().启动验证